## XOR 문제 해결책
XOR 문제의 해결책은 그냥 여러개의 뉴런을 쓰면 된다. 사실 여러 개의 뉴런을 쓰면 어떤 bool 함수던지 풀 수 있다. 여기서는 3개의 뉴런을 쓴다. 코드가 복잡해지는 것을 방지하고자 고수준 API인 **tf.keras**를 이용한다.
```py
import numpy as np
import tensorflow as tf
x = np.array([[1,1],[1,0],[0,1],[0,0]])
y = np.array([0,1,1,0])

model = tf.keras.Sequential([
    tf.keras.layers.Dense(units=2, activation='sigmoid', input_shape=(2,)),
    tf.keras.layers.Dense(units=1, activation='sigmoid')
])

model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1), loss='mse')
model.summary()
```
keras는 딥러닝을 쉽게 계산하기 위한 추상 클래스인 model이 있다.

model에서 가장 많이 쓰는 구조가 `tf.keras.Sequential` 구조이다. 이건 레이어를 일직선으로 배치한 것이다. 이러한 시퀸셜 모델에는 레이어 리스트를 건내주어야 한다. `tf.keras.layers.Dense`는 가장 기본적인 레이어로 레이어의 입력과 출력 사이의 모든 뉴런이 연결되어 있는 형태이다. Dense 안의 `units`는 레이어를 구성하는 뉴런 수를 정의한다. `activation`은 활성화 함수이고 `input_shape`은 시퀸셜 모델의 첫번째 레이어에서만 정의하는 것으로 입력 차원 수를 의미한다.

즉, 위의 코드는 입력 2개를 받아서, 뉴런 2개인 첫번째 레이어에 넘겨주고, 그 레이어는 다시 뉴런 1개인 2번째 레이어에 값을 건내주며, 최종적으로 마지막 레이어의 출력값이 결과가 되는 형태이다.

참고로 모든 레이어는 기본적으로 bias를 포함하고 있기 때문에 `summary`로 값을 출력해보면 입력 파라미터가 +1이 되어 출력된다.

보통 Dense 레이어의 파라미터 수는 $(\text{input size}+1) \times (\text{output size})$로 계산이 가능하다.

마지막으로 `compile` 부분은 모델이 동작할 수 있도록 준비하는 명령이다. `optimizer`는 최적화 함수로 딥러닝 학습식을 정의하는 부분이다. **SGD**는 확률적 경사 하강법의 약자로 일부 샘플을 랜덤으로 골라 그것을 기준으로 경사 하강법을 수행하는 알고리즘이다. 손실은 앞서 살펴본 error의 개념이며 **MSE**는 제곱 평균 제곱근의 약자로 기대 출력에서 실제 출력을 빼고 제곱한 값을 평균으로 하는 것이다. 이 오차를 최소화하는 방향으로 학습이 진행되게 된다.

$$
MSE=\frac{1}{n}\sum_{k=1}^n (y_k-y'_k)^2
$$

여기서 $y'_k$는 출력값이고, $y_k$는 이론적으로 출력을 원하는 값이다.

네트워크를 실제로 학습시키려면 아래처럼 입력하면 된다.
```py
history = model.fit(x, y, epochs=2000, batch_size=1)
```
첫번째 인자는 입력값, 두번째는 출력값, `epochs`는 학습 반복 횟수를 의미하고, `batch_size`는 한 번에 학습하는 데이터의 수이다. 여기서는 1을 입력하여 한번에 하나의 값이 학습되게 한다.

학습이 끝나면 아래와 같이 입력하여 평가를 진행할 수 있다.
```py
model.predict(x)
```
더 많은 정보가 필요하면 가중치 정보를 확인해볼 수 있다. 가중치 정보는 `model.weights`에 저장된다.
