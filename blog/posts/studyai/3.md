## 뉴런 만들기
신경망의 가장 기본이 되는 뉴런을 한번 만들어보자. 이 뉴런은 퍼셉트론이라고도 불린다. 뉴런은 입력을 받아서 계산 후에 출력하는 단순한 구조이다.

신경망은 뉴런이 여러 개 모여 레이어를 구성한 뒤, 이 레이어가 다시 모여 구성된 형태이다.

뉴런은 입력, 가중치, 활성화 함수, 출력으로 구성된다. 입력과 가중치, 출력은 정수나 float가 많이 쓰인다. 활성화함수는 뉴런의 출력값을 정하는 함수이다.

활성화 함수로는 시그모이드, ReLU 등이 많이 사용된다. 각각은 다음과 같다.

$$
\sigma(z)=\frac{1}{1+e^{-z}}, \qquad R(z)=\max(0,z)
$$

신경망 초창기엔 시그모이드가 주로 쓰였지만, 은닉층이 사용되는 딥러닝 시대가 되면서 ReLU가 더 많이 쓰인다. 딥러닝에서 오류 역전파에 시그모이드가 값을 점점 더 작아지게 하는 문제가 보고되었기에 ReLU가 더 좋은 선택이 되게 된다.

### 시그모이드
시그모이드는 아래와 같이 구현할 수 있다.
```py
import math

def sigmoid(x):
    return 1 / (1 + math.exp(-x))
```
입력값을 x, 이상적인 출력값을 y, 가중치를 w라고 하자. 아래와 같이 쓰면 가중치가 적용된 output을 얻을 수 있다.
```py
x = 1
y = 0

w = tf.random.normal([1],0,1)
output = sigmoid(x * w)
print(output)
```

### 경사하강법
여기서 뉴런이란 결국 w 값이다. 이 w를 변화시키는 것이 중요하다. 그 방법이 경사하강법이다. 이것은 w에 입력값과 학습률($\alpha$)과 에러를 곱한 값을 더해주는 것이다.

$$
w^{(i+1)} \gets w^{(i)} + x \cdot \alpha \cdot \text{error}
$$

학습률은 w를 갱신하는 정도이다. 경사하강법이 효과를 발휘하는지는 아래의 코드로 확인해볼 수 있다.
```py
alpha = 0.1
for i in range(1000):
    output = sigmoid(x * w)
    error = y - output
    w = w + x * alpha * error

    if i % 100 == 0:
        print(i, error, output)
```
실행해서 확인해보면 output 값이 점점 더 0에 가까워지는 것을 볼 수 있다.

반대로, `x, y = 0, 1`인 경우를 생각해보자. 이때는 x가 0이기 때문에 값이 제대로 업데이트되지 않는다. 이런 경우를 대비하여, 편향(bias)가 존재하게 된다. 편향은 보편적으로 1이 쓰인다.
```
x, y = 0, 1
alpha = 0.1
w = tf.random.normal([1],0,1)
b = tf.random.normal([1],0,1)

for i in range(1000):
    output = sigmoid(x * w + 1 * b)
    error = y - output
    w = w + x * alpha * error
    b = b + 1 * alpha * error

    if i % 100 == 0:
        print(i, error, output)
```
위는 편향 값을 추가하여, 기존에는 갱신되지 않던 값이 잘 갱신됨을 확인할 수 있다. 편향 값 또한 시간에 따라 값을 갱신해준다.

### AND 신경망
| ㅁㅁ | ㅁㅁ |

